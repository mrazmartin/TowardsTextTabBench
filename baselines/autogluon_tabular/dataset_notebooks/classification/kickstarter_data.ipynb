{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0200d09",
   "metadata": {},
   "source": [
    "### Let's start with downloading the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d0c4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# to reach the dataloader_functions module\n",
    "module_path = os.path.abspath(os.path.join( \"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from dataloader_functions.download_data import download_raw_data\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b8103e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO update this section per dataset\n",
    "dataset_config = {\n",
    "    'dataset_name': 'kockstarter',\n",
    "    'source': 'kaggle', # ['kaggle', 'local', 'openml', 'hf']\n",
    "    'remote_path': 'yashkantharia/kickstarter-campaigns',\n",
    "    'files': ['Kickstarter_projects_Feb19.csv'],\n",
    "    'rename_files': ['kickstarter_data.csv'],\n",
    "    'task': 'clf', # ['reg', 'clf']\n",
    "    'target': 'status',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4336bfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36mInfo:\u001b[0m Dataset already downloaded in \u001b[1;35m/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/../../datasets_files/raw/classification/kockstarter\u001b[0m.\n",
      "Downloaded kockstarter dataset to /home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/../../datasets_files/raw/classification/kockstarter\n"
     ]
    }
   ],
   "source": [
    "if dataset_config['task'] == 'clf':\n",
    "    dataset_subfolder = os.path.join('raw', 'classification', dataset_config['dataset_name']) \n",
    "elif dataset_config['task'] == 'reg':\n",
    "    dataset_subfolder = os.path.join('raw', 'regression', dataset_config['dataset_name'])\n",
    "else:\n",
    "    raise ValueError(f\"Unknown task: {dataset_config['task']}\")\n",
    "\n",
    "# this path needs to be modified based on the location of the notebook\n",
    "download_path = os.path.join(current_dir, '..', '..', 'datasets_files', dataset_subfolder)\n",
    "\n",
    "if download_raw_data(\n",
    "    dataset_config=dataset_config,\n",
    "    download_path=download_path,\n",
    "    force_download=False,\n",
    "    remove_unlisted=True,\n",
    ") is not None:\n",
    "    print(f\"Downloaded {dataset_config['dataset_name']} dataset to {download_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d886637",
   "metadata": {},
   "source": [
    "### Now we need to preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48bcaebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader_functions.load_and_pp_raw_data import _drop_empty_columns, _drop_single_value_columns\n",
    "from dataloader_functions.utils.data_2_df import read_any_to_df\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da377310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: we will downsample the dataset to ~24k samples\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path\n",
    "current_dir = os.getcwd()\n",
    "download_path = os.path.join(current_dir, '..', '..', 'datasets_files', dataset_subfolder)\n",
    "\n",
    "# File paths\n",
    "train_path = os.path.join(download_path, dataset_config['rename_files'][0])\n",
    "\n",
    "# Load safely, skipping bad lines\n",
    "train_df = pd.read_csv(train_path, on_bad_lines='skip', engine='python')\n",
    "\n",
    "# Downsample with fallback if not enough rows\n",
    "train_sample_size = min(len(train_df), 24000)\n",
    "\n",
    "train_df = train_df.sample(n=train_sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save back (overwrite the originals)\n",
    "train_df.to_csv(train_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0861820c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/../../datasets_files/raw/classification/kockstarter/kickstarter_data.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>currency</th>\n",
       "      <th>main_category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>launched_at</th>\n",
       "      <th>deadline</th>\n",
       "      <th>duration</th>\n",
       "      <th>goal_usd</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>blurb_length</th>\n",
       "      <th>name_length</th>\n",
       "      <th>status</th>\n",
       "      <th>start_month</th>\n",
       "      <th>end_month</th>\n",
       "      <th>start_Q</th>\n",
       "      <th>end_Q</th>\n",
       "      <th>usd_pledged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>601819466</td>\n",
       "      <td>Rocketmen the Webseries</td>\n",
       "      <td>USD</td>\n",
       "      <td>film &amp; video</td>\n",
       "      <td>Webseries</td>\n",
       "      <td>2013-07-11 16:27:41</td>\n",
       "      <td>2013-08-10 16:27:41</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>US</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>successful</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Q3</td>\n",
       "      <td>Q3</td>\n",
       "      <td>33039.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>259627750</td>\n",
       "      <td>Explore The Uncanny Valley with The Horse-Eyed...</td>\n",
       "      <td>USD</td>\n",
       "      <td>music</td>\n",
       "      <td>Country &amp; Folk</td>\n",
       "      <td>2018-06-12 20:23:58</td>\n",
       "      <td>2018-07-06 02:30:00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>Providence</td>\n",
       "      <td>RI</td>\n",
       "      <td>US</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>successful</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>Q2</td>\n",
       "      <td>Q3</td>\n",
       "      <td>6490.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1805285154</td>\n",
       "      <td>Latch Key Kids Podcast</td>\n",
       "      <td>USD</td>\n",
       "      <td>journalism</td>\n",
       "      <td>Audio</td>\n",
       "      <td>2014-07-11 23:29:09</td>\n",
       "      <td>2014-08-10 23:29:09</td>\n",
       "      <td>30.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>Gainesville</td>\n",
       "      <td>GA</td>\n",
       "      <td>US</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>successful</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Q3</td>\n",
       "      <td>Q3</td>\n",
       "      <td>332.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               name currency  \\\n",
       "0   601819466                            Rocketmen the Webseries      USD   \n",
       "1   259627750  Explore The Uncanny Valley with The Horse-Eyed...      USD   \n",
       "2  1805285154                             Latch Key Kids Podcast      USD   \n",
       "\n",
       "  main_category    sub_category          launched_at             deadline  \\\n",
       "0  film & video       Webseries  2013-07-11 16:27:41  2013-08-10 16:27:41   \n",
       "1         music  Country & Folk  2018-06-12 20:23:58  2018-07-06 02:30:00   \n",
       "2    journalism           Audio  2014-07-11 23:29:09  2014-08-10 23:29:09   \n",
       "\n",
       "   duration  goal_usd         city state country  blurb_length  name_length  \\\n",
       "0      30.0   30000.0      Seattle    WA      US            22            3   \n",
       "1      23.0    5000.0   Providence    RI      US            20            8   \n",
       "2      30.0     250.0  Gainesville    GA      US            14            4   \n",
       "\n",
       "       status  start_month  end_month start_Q end_Q  usd_pledged  \n",
       "0  successful            7          8      Q3    Q3     33039.00  \n",
       "1  successful            6          7      Q2    Q3      6490.00  \n",
       "2  successful            7          8      Q3    Q3       332.37  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset_files_df = []\n",
    "\n",
    "if dataset_config['rename_files'] is None or len(dataset_config['rename_files']) == 0:\n",
    "    dataset_config['rename_files'] = dataset_config['files']\n",
    "\n",
    "for file in dataset_config['rename_files']:\n",
    "\n",
    "    file_location = os.path.join(download_path, file)\n",
    "\n",
    "    print(f\"Loading {file_location}\")\n",
    "\n",
    "    dataset_files_df.append(read_any_to_df(file_location))\n",
    "\n",
    "# example of the loaded df data:\n",
    "pd.set_option('display.max_columns', None)\n",
    "dataset_files_df[0].head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d099b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped: Index([], dtype='object')\n",
      "Dataframe shape before/afrer cleaning: (24000, 20) / (23660, 20)\n"
     ]
    }
   ],
   "source": [
    "## Run some basic data cleaning\n",
    "\n",
    "dataset_files_gen_cleaned = []\n",
    "missing_ratio_threshold = 0.5 # TODO the threshold can be changed\n",
    "\n",
    "for df_file in dataset_files_df:\n",
    "    df_size = df_file.shape\n",
    "    # 1. Drop columns with more than 50% missing values\n",
    "    df_file = _drop_empty_columns(df_file, threshold=missing_ratio_threshold)   \n",
    "    # 2. Drop columns with only one unique value\n",
    "    df_file = _drop_single_value_columns(df_file)\n",
    "    # 3. remove duplicates\n",
    "    df_file = df_file.drop_duplicates()\n",
    "    # 4. remove rows with missing target values\n",
    "    df_file = df_file[df_file[dataset_config['target']].notna()]\n",
    "    # 5. drop unnamed columns\n",
    "    df_file = df_file.loc[:, ~df_file.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    dataset_files_gen_cleaned.append(df_file)\n",
    "\n",
    "    print(f\"Dataframe shape before/afrer cleaning: {df_size} / {df_file.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0a2a11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape before/afrer by-hand cleaning: (23660, 20) / (23660, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>currency</th>\n",
       "      <th>main_category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>launched_at</th>\n",
       "      <th>deadline</th>\n",
       "      <th>duration</th>\n",
       "      <th>goal_usd</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>blurb_length</th>\n",
       "      <th>name_length</th>\n",
       "      <th>status</th>\n",
       "      <th>start_month</th>\n",
       "      <th>end_month</th>\n",
       "      <th>start_Q</th>\n",
       "      <th>end_Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rocketmen the Webseries</td>\n",
       "      <td>USD</td>\n",
       "      <td>film &amp; video</td>\n",
       "      <td>Webseries</td>\n",
       "      <td>2013-07-11 16:27:41</td>\n",
       "      <td>2013-08-10 16:27:41</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>US</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>successful</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Q3</td>\n",
       "      <td>Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explore The Uncanny Valley with The Horse-Eyed...</td>\n",
       "      <td>USD</td>\n",
       "      <td>music</td>\n",
       "      <td>Country &amp; Folk</td>\n",
       "      <td>2018-06-12 20:23:58</td>\n",
       "      <td>2018-07-06 02:30:00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>Providence</td>\n",
       "      <td>RI</td>\n",
       "      <td>US</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>successful</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>Q2</td>\n",
       "      <td>Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Latch Key Kids Podcast</td>\n",
       "      <td>USD</td>\n",
       "      <td>journalism</td>\n",
       "      <td>Audio</td>\n",
       "      <td>2014-07-11 23:29:09</td>\n",
       "      <td>2014-08-10 23:29:09</td>\n",
       "      <td>30.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>Gainesville</td>\n",
       "      <td>GA</td>\n",
       "      <td>US</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>successful</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Q3</td>\n",
       "      <td>Q3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name currency main_category  \\\n",
       "0                            Rocketmen the Webseries      USD  film & video   \n",
       "1  Explore The Uncanny Valley with The Horse-Eyed...      USD         music   \n",
       "2                             Latch Key Kids Podcast      USD    journalism   \n",
       "\n",
       "     sub_category          launched_at             deadline  duration  \\\n",
       "0       Webseries  2013-07-11 16:27:41  2013-08-10 16:27:41      30.0   \n",
       "1  Country & Folk  2018-06-12 20:23:58  2018-07-06 02:30:00      23.0   \n",
       "2           Audio  2014-07-11 23:29:09  2014-08-10 23:29:09      30.0   \n",
       "\n",
       "   goal_usd         city state country  blurb_length  name_length      status  \\\n",
       "0   30000.0      Seattle    WA      US            22            3  successful   \n",
       "1    5000.0   Providence    RI      US            20            8  successful   \n",
       "2     250.0  Gainesville    GA      US            14            4  successful   \n",
       "\n",
       "   start_month  end_month start_Q end_Q  \n",
       "0            7          8      Q3    Q3  \n",
       "1            6          7      Q2    Q3  \n",
       "2            7          8      Q3    Q3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO: Now run custom data cleaning -> remove non-essential columns\n",
    "\n",
    "cols_to_drop = ['usd_pledged', 'id'] # to remove target leakage\n",
    "\n",
    "dataset_files_cleaned = []\n",
    "\n",
    "# assuming for multiple files we still want to drop the same columns\n",
    "for df_file in dataset_files_gen_cleaned:\n",
    "    df_size = df_file.shape\n",
    "    for col in cols_to_drop:\n",
    "        if col in df_file.columns:\n",
    "            df_file.drop(col, axis=1, inplace=True)\n",
    "        else:\n",
    "            print(f\"Column {col} not found in dataframe\")\n",
    "\n",
    "    dataset_files_cleaned.append(df_file)    \n",
    "    print(f\"Dataframe shape before/afrer by-hand cleaning: {df_size} / {df_file.shape}\")\n",
    "\n",
    "dataset_files_cleaned[0].head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d90a681b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape before custom cleaning: (23660, 18)\n",
      "Dataframe shape after custom cleaning: (23660, 18)\n"
     ]
    }
   ],
   "source": [
    "# TODO: some custom data cleaning\n",
    "\n",
    "import copy \n",
    "import pandas as pd\n",
    "\n",
    "tmp_df = copy.deepcopy(dataset_files_cleaned)\n",
    "dataset_files_by_hand_cleaned = []\n",
    "\n",
    "for df_file in dataset_files_cleaned:\n",
    "    print(f\"Dataframe shape before custom cleaning: {df_file.shape}\")\n",
    "    \n",
    "    # 1. Convert to datetime safely\n",
    "    df_file['launched_at'] = pd.to_datetime(df_file['launched_at'], errors='coerce')\n",
    "    df_file['deadline'] = pd.to_datetime(df_file['deadline'], errors='coerce')\n",
    "    \n",
    "    # 2. Convert datetime to Unix timestamp as float (seconds since epoch)\n",
    "    df_file['launched_at'] = df_file['launched_at'].astype('int64') / 1e9  # nanoseconds to seconds\n",
    "    df_file['deadline'] = df_file['deadline'].astype('int64') / 1e9\n",
    "\n",
    "    print(f\"Dataframe shape after custom cleaning: {df_file.shape}\")\n",
    "\n",
    "    dataset_files_by_hand_cleaned.append(df_file)\n",
    "\n",
    "# Reset the dataframe list to pre-cleaning version\n",
    "dataset_files_cleaned = tmp_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581b17e1",
   "metadata": {},
   "source": [
    "### Now it is time to visualize our changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1500c210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>currency</th>\n",
       "      <th>main_category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>launched_at</th>\n",
       "      <th>deadline</th>\n",
       "      <th>duration</th>\n",
       "      <th>goal_usd</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>blurb_length</th>\n",
       "      <th>name_length</th>\n",
       "      <th>status</th>\n",
       "      <th>start_month</th>\n",
       "      <th>end_month</th>\n",
       "      <th>start_Q</th>\n",
       "      <th>end_Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rocketmen the Webseries</td>\n",
       "      <td>USD</td>\n",
       "      <td>film &amp; video</td>\n",
       "      <td>Webseries</td>\n",
       "      <td>1.373560e+09</td>\n",
       "      <td>1.376152e+09</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>US</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>successful</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Q3</td>\n",
       "      <td>Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explore The Uncanny Valley with The Horse-Eyed...</td>\n",
       "      <td>USD</td>\n",
       "      <td>music</td>\n",
       "      <td>Country &amp; Folk</td>\n",
       "      <td>1.528835e+09</td>\n",
       "      <td>1.530844e+09</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>Providence</td>\n",
       "      <td>RI</td>\n",
       "      <td>US</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>successful</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>Q2</td>\n",
       "      <td>Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Latch Key Kids Podcast</td>\n",
       "      <td>USD</td>\n",
       "      <td>journalism</td>\n",
       "      <td>Audio</td>\n",
       "      <td>1.405121e+09</td>\n",
       "      <td>1.407713e+09</td>\n",
       "      <td>30.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>Gainesville</td>\n",
       "      <td>GA</td>\n",
       "      <td>US</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>successful</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Q3</td>\n",
       "      <td>Q3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name currency main_category  \\\n",
       "0                            Rocketmen the Webseries      USD  film & video   \n",
       "1  Explore The Uncanny Valley with The Horse-Eyed...      USD         music   \n",
       "2                             Latch Key Kids Podcast      USD    journalism   \n",
       "\n",
       "     sub_category   launched_at      deadline  duration  goal_usd  \\\n",
       "0       Webseries  1.373560e+09  1.376152e+09      30.0   30000.0   \n",
       "1  Country & Folk  1.528835e+09  1.530844e+09      23.0    5000.0   \n",
       "2           Audio  1.405121e+09  1.407713e+09      30.0     250.0   \n",
       "\n",
       "          city state country  blurb_length  name_length      status  \\\n",
       "0      Seattle    WA      US            22            3  successful   \n",
       "1   Providence    RI      US            20            8  successful   \n",
       "2  Gainesville    GA      US            14            4  successful   \n",
       "\n",
       "   start_month  end_month start_Q end_Q  \n",
       "0            7          8      Q3    Q3  \n",
       "1            6          7      Q2    Q3  \n",
       "2            7          8      Q3    Q3  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "dataset_files_by_hand_cleaned[0].head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f71269d",
   "metadata": {},
   "source": [
    "Let's figure out which columns should be viewed as categorical / numerical / textual\n",
    "\n",
    "we can start with a simple heuristic:\n",
    "1. numerical is everything which\n",
    "    - keeps most of its character length after non-numeral strip\n",
    "    - has about the same number of unique values after the strip\n",
    "    + for the purpose of keeping \"semantic information\", hand picked columns can be viewed also as non-numerical, that is not the default benchmark approach though\n",
    "\n",
    "2. categorical is everthing non numerical, which can be then divided into N (where N << Num instances) unique categories\n",
    "\n",
    "3. textual is everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "436f9ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold for categorical vs textual: 50\n",
      "Numerical columns (4): ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "Categorical columns (10): ['currency', 'main_category', 'country', 'blurb_length', 'name_length', 'status', 'start_month', 'end_month', 'start_Q', 'end_Q']\n",
      "Textual columns (4): ['name', 'sub_category', 'city', 'state']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def is_mostly_numeric(series, length_threshold=0.5, unique_threshold=0.8):\n",
    "    \"\"\"Check if column is mostly numeric after stripping non-numeric chars.\"\"\"\n",
    "    stripped = series.astype(str).str.replace(r\"[^\\d\\.\\-]\", \"\", regex=True)\n",
    "    \n",
    "    original_len = series.astype(str).str.len().replace(0, 1)  # avoid div by zero\n",
    "    length_ratio = (stripped.str.len() / original_len).mean()\n",
    "\n",
    "    unique_ratio = stripped.nunique(dropna=False) / max(series.nunique(dropna=False), 1)\n",
    "\n",
    "    return length_ratio > length_threshold and unique_ratio > unique_threshold\n",
    "\n",
    "def classify_columns(df, unique_ratio_threshold=None, explicit_nunique_threshold=None):\n",
    "    \"\"\"\n",
    "    Classify dataframe columns into numerical, categorical, textual.\n",
    "    - Binary categorical (2 unique values) is considered categorical.\n",
    "    \"\"\"\n",
    "    n_rows = len(df)\n",
    "\n",
    "    # Determine threshold for categorical vs textual\n",
    "    if explicit_nunique_threshold is not None:\n",
    "        nunique_threshold = explicit_nunique_threshold\n",
    "    elif unique_ratio_threshold is not None:\n",
    "        nunique_threshold = int(unique_ratio_threshold * n_rows)\n",
    "    else:\n",
    "        nunique_threshold = int(0.05 * n_rows)  # default 5%\n",
    "\n",
    "    nunique_threshold = max(10, nunique_threshold)  # safeguard\n",
    "    print(f\"Threshold for categorical vs textual: {nunique_threshold}\")\n",
    "\n",
    "    numerical_cols = []\n",
    "    categorical_cols = []\n",
    "    textual_cols = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "        nunique = series.nunique(dropna=False)\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(series):\n",
    "            if nunique <= nunique_threshold:\n",
    "                categorical_cols.append(col)\n",
    "            else:\n",
    "                numerical_cols.append(col)\n",
    "        \n",
    "        elif pd.api.types.is_string_dtype(series) or pd.api.types.is_object_dtype(series):\n",
    "            if is_mostly_numeric(series):\n",
    "                numerical_cols.append(col)\n",
    "            else:\n",
    "                if nunique <= nunique_threshold:\n",
    "                    categorical_cols.append(col)\n",
    "                else:\n",
    "                    textual_cols.append(col)\n",
    "        else:\n",
    "            print(f\"⚠️ Unhandled column type: '{col}' (dtype={series.dtype})\")\n",
    "\n",
    "    print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
    "    print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "    print(f\"Textual columns ({len(textual_cols)}): {textual_cols}\")\n",
    "\n",
    "    return numerical_cols, categorical_cols, textual_cols\n",
    "\n",
    "# Example usage:\n",
    "for df_file in dataset_files_by_hand_cleaned:\n",
    "    # Ratio-based (5% of rows)\n",
    "    # umerical_cols, categorical_cols, textual_cols = classify_columns(df_file, unique_ratio_threshold=0.05)\n",
    "    print(\"\")\n",
    "    # OR explicit value (e.g., anything <= 50 is categorical)\n",
    "    numerical_cols, categorical_cols, textual_cols = classify_columns(df_file, explicit_nunique_threshold=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6499589b",
   "metadata": {},
   "source": [
    "Now let's just try to visualize the kept features, their example values, and their cat/num/text allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afffbdb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column Name</th>\n",
       "      <th>Example Value</th>\n",
       "      <th>Type</th>\n",
       "      <th># Categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>name</td>\n",
       "      <td>Rocketmen the Webseries</td>\n",
       "      <td>textual</td>\n",
       "      <td>23614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>currency</td>\n",
       "      <td>USD</td>\n",
       "      <td>categorical</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>main_category</td>\n",
       "      <td>film &amp; video</td>\n",
       "      <td>categorical</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub_category</td>\n",
       "      <td>Webseries</td>\n",
       "      <td>textual</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>launched_at</td>\n",
       "      <td>1373560061.0</td>\n",
       "      <td>numerical</td>\n",
       "      <td>~ 23623 ~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deadline</td>\n",
       "      <td>1376152061.0</td>\n",
       "      <td>numerical</td>\n",
       "      <td>~ 23248 ~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>duration</td>\n",
       "      <td>30.0</td>\n",
       "      <td>numerical</td>\n",
       "      <td>~ 82 ~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>goal_usd</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>numerical</td>\n",
       "      <td>~ 2713 ~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>city</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>textual</td>\n",
       "      <td>4058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>state</td>\n",
       "      <td>WA</td>\n",
       "      <td>textual</td>\n",
       "      <td>555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>country</td>\n",
       "      <td>US</td>\n",
       "      <td>categorical</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>blurb_length</td>\n",
       "      <td>22</td>\n",
       "      <td>categorical</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>name_length</td>\n",
       "      <td>3</td>\n",
       "      <td>categorical</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>status</td>\n",
       "      <td>successful</td>\n",
       "      <td>categorical</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>start_month</td>\n",
       "      <td>7</td>\n",
       "      <td>categorical</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>end_month</td>\n",
       "      <td>8</td>\n",
       "      <td>categorical</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>start_Q</td>\n",
       "      <td>Q3</td>\n",
       "      <td>categorical</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>end_Q</td>\n",
       "      <td>Q3</td>\n",
       "      <td>categorical</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Column Name            Example Value         Type # Categories\n",
       "0            name  Rocketmen the Webseries      textual        23614\n",
       "1        currency                      USD  categorical           14\n",
       "2   main_category             film & video  categorical           15\n",
       "3    sub_category                Webseries      textual          159\n",
       "4     launched_at             1373560061.0    numerical    ~ 23623 ~\n",
       "5        deadline             1376152061.0    numerical    ~ 23248 ~\n",
       "6        duration                     30.0    numerical       ~ 82 ~\n",
       "7        goal_usd                  30000.0    numerical     ~ 2713 ~\n",
       "8            city                  Seattle      textual         4058\n",
       "9           state                       WA      textual          555\n",
       "10        country                       US  categorical           22\n",
       "11   blurb_length                       22  categorical           34\n",
       "12    name_length                        3  categorical           18\n",
       "13         status               successful  categorical            2\n",
       "14    start_month                        7  categorical           12\n",
       "15      end_month                        8  categorical           12\n",
       "16        start_Q                       Q3  categorical            4\n",
       "17          end_Q                       Q3  categorical            4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary = []\n",
    "for col in df_file.columns:\n",
    "    if col in categorical_cols:\n",
    "        col_type = \"categorical\"\n",
    "        num_categories = df_file[col].nunique(dropna=True)\n",
    "        num_categories_display = int(num_categories)\n",
    "    elif col in textual_cols:\n",
    "        col_type = \"textual\"\n",
    "        num_categories = df_file[col].nunique(dropna=True)\n",
    "        num_categories_display = int(num_categories)\n",
    "    elif col in numerical_cols:\n",
    "        col_type = \"numerical\"\n",
    "        num_categories = df_file[col].nunique(dropna=True)\n",
    "        num_categories_display = '~ ' + str(num_categories) + ' ~'\n",
    "    else:\n",
    "        col_type = \"unknown\"\n",
    "        num_categories_display = '--'\n",
    "\n",
    "    example = df_file[col].dropna().iloc[0] if df_file[col].dropna().size > 0 else None\n",
    "    summary.append({\n",
    "        'Column Name': col,\n",
    "        'Example Value': str(example),\n",
    "        'Type': col_type,\n",
    "        '# Categories': num_categories_display\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "\n",
    "# TODO: by hand changes of the type and category count -> e.g. for 'Location':\n",
    "#    # Post-processing: override the type and category count for 'Location'\n",
    "#    # summary_df.loc[summary_df['Column Name'] == 'Location', 'Type'] = 'textual'\n",
    "#    # num_categories = df_file['Location'].nunique(dropna=True)\n",
    "#    # summary_df.loc[summary_df['Column Name'] == 'Location', '# Categories'] = int(num_categories)\n",
    "\n",
    "display(summary_df)  # Or print(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ef8954",
   "metadata": {},
   "source": [
    "### Saving the processed data and loading it back up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fe5eab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/../../datasets_files/raw/classification/kockstarter/kickstarter_data_processed.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Handle rename_files fallback\n",
    "if dataset_config['rename_files'] is None or len(dataset_config['rename_files']) == 0:\n",
    "    dataset_config['rename_files'] = dataset_config['files']\n",
    "\n",
    "for i, df_file in enumerate(dataset_files_by_hand_cleaned):\n",
    "    df_with_meta = df_file.copy()\n",
    "\n",
    "    # Get corresponding file name\n",
    "    file_name = dataset_config['rename_files'][i]\n",
    "    file_base = os.path.splitext(file_name)[0]\n",
    "\n",
    "    # Try assigning multi-index header from summary\n",
    "    try:\n",
    "        df_with_meta.columns = pd.MultiIndex.from_frame(\n",
    "            summary_df[['Column Name', 'Type', '# Categories']]\n",
    "        )\n",
    "        local_summary = summary_df.copy()\n",
    "    except ValueError:\n",
    "        local_summary = summary_df.copy()\n",
    "        if dataset_config['target'] in local_summary['Column Name'].values:\n",
    "            local_summary = local_summary[local_summary['Column Name'] != dataset_config['target']]\n",
    "            df_with_meta.columns = pd.MultiIndex.from_frame(\n",
    "                local_summary[['Column Name', 'Type', '# Categories']]\n",
    "            )\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    # Construct and save everything together\n",
    "    output_filename = f\"{file_base}_processed.pkl\"\n",
    "    output_path = os.path.join(download_path, output_filename)\n",
    "\n",
    "    save_bundle = {\n",
    "        'data': df_with_meta,\n",
    "        'summary': local_summary,\n",
    "        'config': dataset_config\n",
    "    }\n",
    "\n",
    "    pd.to_pickle(save_bundle, output_path)\n",
    "    print(f\"Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c7bcd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KICKSTARTER_DATA ===\n",
      "Loaded config keys: ['dataset_name', 'source', 'remote_path', 'files', 'rename_files', 'task', 'target']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column Name</th>\n",
       "      <th>Type</th>\n",
       "      <th># Categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>name</td>\n",
       "      <td>textual</td>\n",
       "      <td>23614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>currency</td>\n",
       "      <td>categorical</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>main_category</td>\n",
       "      <td>categorical</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub_category</td>\n",
       "      <td>textual</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>launched_at</td>\n",
       "      <td>numerical</td>\n",
       "      <td>~ 23623 ~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deadline</td>\n",
       "      <td>numerical</td>\n",
       "      <td>~ 23248 ~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>duration</td>\n",
       "      <td>numerical</td>\n",
       "      <td>~ 82 ~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>goal_usd</td>\n",
       "      <td>numerical</td>\n",
       "      <td>~ 2713 ~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>city</td>\n",
       "      <td>textual</td>\n",
       "      <td>4058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>state</td>\n",
       "      <td>textual</td>\n",
       "      <td>555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>country</td>\n",
       "      <td>categorical</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>blurb_length</td>\n",
       "      <td>categorical</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>name_length</td>\n",
       "      <td>categorical</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>status</td>\n",
       "      <td>categorical</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>start_month</td>\n",
       "      <td>categorical</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>end_month</td>\n",
       "      <td>categorical</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>start_Q</td>\n",
       "      <td>categorical</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>end_Q</td>\n",
       "      <td>categorical</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Column Name         Type # Categories\n",
       "0            name      textual        23614\n",
       "1        currency  categorical           14\n",
       "2   main_category  categorical           15\n",
       "3    sub_category      textual          159\n",
       "4     launched_at    numerical    ~ 23623 ~\n",
       "5        deadline    numerical    ~ 23248 ~\n",
       "6        duration    numerical       ~ 82 ~\n",
       "7        goal_usd    numerical     ~ 2713 ~\n",
       "8            city      textual         4058\n",
       "9           state      textual          555\n",
       "10        country  categorical           22\n",
       "11   blurb_length  categorical           34\n",
       "12    name_length  categorical           18\n",
       "13         status  categorical            2\n",
       "14    start_month  categorical           12\n",
       "15      end_month  categorical           12\n",
       "16        start_Q  categorical            4\n",
       "17          end_Q  categorical            4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Column Name</th>\n",
       "      <th>name</th>\n",
       "      <th>currency</th>\n",
       "      <th>main_category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>launched_at</th>\n",
       "      <th>deadline</th>\n",
       "      <th>duration</th>\n",
       "      <th>goal_usd</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>blurb_length</th>\n",
       "      <th>name_length</th>\n",
       "      <th>status</th>\n",
       "      <th>start_month</th>\n",
       "      <th>end_month</th>\n",
       "      <th>start_Q</th>\n",
       "      <th>end_Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rocketmen the Webseries</td>\n",
       "      <td>USD</td>\n",
       "      <td>film &amp; video</td>\n",
       "      <td>Webseries</td>\n",
       "      <td>1.373560e+09</td>\n",
       "      <td>1.376152e+09</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>US</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>successful</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Q3</td>\n",
       "      <td>Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explore The Uncanny Valley with The Horse-Eyed...</td>\n",
       "      <td>USD</td>\n",
       "      <td>music</td>\n",
       "      <td>Country &amp; Folk</td>\n",
       "      <td>1.528835e+09</td>\n",
       "      <td>1.530844e+09</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>Providence</td>\n",
       "      <td>RI</td>\n",
       "      <td>US</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>successful</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>Q2</td>\n",
       "      <td>Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Latch Key Kids Podcast</td>\n",
       "      <td>USD</td>\n",
       "      <td>journalism</td>\n",
       "      <td>Audio</td>\n",
       "      <td>1.405121e+09</td>\n",
       "      <td>1.407713e+09</td>\n",
       "      <td>30.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>Gainesville</td>\n",
       "      <td>GA</td>\n",
       "      <td>US</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>successful</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Q3</td>\n",
       "      <td>Q3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Column Name                                               name currency  \\\n",
       "0                                      Rocketmen the Webseries      USD   \n",
       "1            Explore The Uncanny Valley with The Horse-Eyed...      USD   \n",
       "2                                       Latch Key Kids Podcast      USD   \n",
       "\n",
       "Column Name main_category    sub_category   launched_at      deadline  \\\n",
       "0            film & video       Webseries  1.373560e+09  1.376152e+09   \n",
       "1                   music  Country & Folk  1.528835e+09  1.530844e+09   \n",
       "2              journalism           Audio  1.405121e+09  1.407713e+09   \n",
       "\n",
       "Column Name  duration  goal_usd         city state country  blurb_length  \\\n",
       "0                30.0   30000.0      Seattle    WA      US            22   \n",
       "1                23.0    5000.0   Providence    RI      US            20   \n",
       "2                30.0     250.0  Gainesville    GA      US            14   \n",
       "\n",
       "Column Name  name_length      status  start_month  end_month start_Q end_Q  \n",
       "0                      3  successful            7          8      Q3    Q3  \n",
       "1                      8  successful            6          7      Q2    Q3  \n",
       "2                      4  successful            7          8      Q3    Q3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Loop through processed files in rename_files\n",
    "for file_name in dataset_config['rename_files']:\n",
    "    # Remove .csv or .tsv extension to get the base name\n",
    "    file_base = os.path.splitext(file_name)[0]\n",
    "    processed_filename = f\"{file_base}_processed.pkl\"\n",
    "    processed_path = os.path.join(download_path, processed_filename)\n",
    "\n",
    "    # Load the bundled dictionary (data + summary + config)\n",
    "    bundle = pd.read_pickle(processed_path)\n",
    "\n",
    "    # Extract components\n",
    "    loaded_df = bundle['data']\n",
    "    summary_df = bundle['summary']\n",
    "    loaded_config = bundle['config']\n",
    "\n",
    "    print(f\"\\n=== {file_base.upper()} ===\")\n",
    "    print(f\"Loaded config keys: {list(loaded_config.keys())}\")\n",
    "\n",
    "    # Show metadata\n",
    "    meta_df = pd.DataFrame(loaded_df.columns.tolist(), columns=['Column Name', 'Type', '# Categories'])\n",
    "    display(meta_df)\n",
    "\n",
    "    # Flatten for modeling\n",
    "    loaded_df.columns = loaded_df.columns.get_level_values(0)\n",
    "    display(loaded_df.head(n=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa05d76",
   "metadata": {},
   "source": [
    "### Bonus insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fec69c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, \"Distribution of 'status' target\")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show data imbalance:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the distribution of player_class with counts\n",
    "value_counts = loaded_df[dataset_config['target']].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "value_counts.plot(kind='bar')\n",
    "\n",
    "# Add counts as text labels on top of bars\n",
    "for i, count in enumerate(value_counts):\n",
    "    plt.text(i, count, str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.title(f\"Distribution of '{dataset_config['target']}' target\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb94bc2b-6c7e-4870-9c3b-170e5904b79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250522_155233\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.0\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2\n",
      "CPU Count:          64\n",
      "Memory Avail:       373.27 GB / 503.54 GB (74.1%)\n",
      "Disk Space Avail:   33782.25 GB / 51214.59 GB (66.0%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[WRAPPER] Running stratified downsampling mode for task: clf\n",
      "\u001b[1;36mInfo:\u001b[0m \n",
      "[INFO] Downsampling dataframe: kockstarter (original rows: 23660)\n",
      "\u001b[1;33mInfo:\u001b[0m Trying to sample ~1500 rows per class (total=3000)\n",
      "\u001b[1;36mInfo:\u001b[0m Final downsampled dataset has 3000 rows. Per class counts: [successful: 1500, failed: 1500]\n",
      "\n",
      "Downsampled 3000 rows for kockstarter dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 360s\n",
      "AutoGluon will save models to \"/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/AutogluonModels/ag-20250522_155233\"\n",
      "Train Data Rows:    2400\n",
      "Train Data Columns: 17\n",
      "Label Column:       status\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = successful, class 0 = failed\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (successful) vs negative (failed) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    382290.15 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.49 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['name']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 33\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])        : 4 | ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "\t\t('int', [])          : 4 | ['blurb_length', 'name_length', 'start_month', 'end_month']\n",
      "\t\t('object', [])       : 8 | ['currency', 'main_category', 'sub_category', 'city', 'state', ...]\n",
      "\t\t('object', ['text']) : 1 | ['name']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  8 | ['currency', 'main_category', 'sub_category', 'city', 'state', ...]\n",
      "\t\t('float', [])                       :  4 | ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "\t\t('int', [])                         :  4 | ['blurb_length', 'name_length', 'start_month', 'end_month']\n",
      "\t\t('int', ['binned', 'text_special']) : 20 | ['name.char_count', 'name.word_count', 'name.capital_ratio', 'name.lower_ratio', 'name.digit_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 34 | ['__nlp__.album', '__nlp__.an', '__nlp__.and', '__nlp__.art', '__nlp__.book', ...]\n",
      "\t1.4s = Fit runtime\n",
      "\t17 features in original data used to generate 70 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.38 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.4s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 358.59s of the 358.57s of remaining time.\n",
      "\t0.5913\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 358.24s of the 358.22s of remaining time.\n",
      "\t0.5704\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 358.13s of the 358.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7817\t = Validation score   (accuracy)\n",
      "\t50.0s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 288.07s of the 288.05s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7871\t = Validation score   (accuracy)\n",
      "\t38.68s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 246.53s of the 246.51s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6963\t = Validation score   (accuracy)\n",
      "\t1.52s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 244.38s of the 244.36s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.7\t = Validation score   (accuracy)\n",
      "\t1.35s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 242.45s of the 242.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.12%)\n",
      "2025-05-22 17:54:35,687\tERROR serialization.py:462 -- Failed to unpickle serialized exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tSystem error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 931, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.RaySystemError: System error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 237.82s of the 237.80s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6813\t = Validation score   (accuracy)\n",
      "\t1.3s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 235.84s of the 235.82s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6854\t = Validation score   (accuracy)\n",
      "\t1.29s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 233.89s of the 233.87s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.765\t = Validation score   (accuracy)\n",
      "\t109.91s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 121.05s of the 121.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.01%)\n",
      "\t0.7867\t = Validation score   (accuracy)\n",
      "\t38.91s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 79.42s of the 79.40s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7154\t = Validation score   (accuracy)\n",
      "\t110.0s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 358.60s of the -33.56s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.7871\t = Validation score   (accuracy)\n",
      "\t0.3s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 393.94s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3116.8 rows/s (300 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2400 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/AutogluonModels/ag-20250522_155233\")\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250522_155908\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.0\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2\n",
      "CPU Count:          64\n",
      "Memory Avail:       372.70 GB / 503.54 GB (74.0%)\n",
      "Disk Space Avail:   33781.88 GB / 51214.59 GB (66.0%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ... Time limit = 360s\n",
      "AutoGluon will save models to \"/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/AutogluonModels/ag-20250522_155908\"\n",
      "Train Data Rows:    2400\n",
      "Train Data Columns: 13\n",
      "Label Column:       status\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = successful, class 0 = failed\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (successful) vs negative (failed) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    381641.79 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.84 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 4 | ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "\t\t('int', [])    : 4 | ['blurb_length', 'name_length', 'start_month', 'end_month']\n",
      "\t\t('object', []) : 5 | ['currency', 'main_category', 'country', 'start_Q', 'end_Q']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 5 | ['currency', 'main_category', 'country', 'start_Q', 'end_Q']\n",
      "\t\t('float', [])    : 4 | ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "\t\t('int', [])      : 4 | ['blurb_length', 'name_length', 'start_month', 'end_month']\n",
      "\t0.1s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 359.86s of the 359.84s of remaining time.\n",
      "\t0.5913\t = Validation score   (accuracy)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 359.76s of the 359.74s of remaining time.\n",
      "\t0.5704\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 359.68s of the 359.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7183\t = Validation score   (accuracy)\n",
      "\t28.38s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 328.68s of the 328.66s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7158\t = Validation score   (accuracy)\n",
      "\t26.63s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 299.54s of the 299.51s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6517\t = Validation score   (accuracy)\n",
      "\t1.2s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 297.80s of the 297.78s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6637\t = Validation score   (accuracy)\n",
      "\t1.16s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 296.13s of the 296.10s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.10%)\n",
      "2025-05-22 18:00:16,920\tERROR serialization.py:462 -- Failed to unpickle serialized exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tSystem error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 931, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.RaySystemError: System error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 291.20s of the 291.18s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6467\t = Validation score   (accuracy)\n",
      "\t1.15s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 289.40s of the 289.38s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6388\t = Validation score   (accuracy)\n",
      "\t1.18s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 287.56s of the 287.53s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.6963\t = Validation score   (accuracy)\n",
      "\t97.15s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 187.83s of the 187.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.6967\t = Validation score   (accuracy)\n",
      "\t33.49s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 150.57s of the 150.55s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.6917\t = Validation score   (accuracy)\n",
      "\t139.33s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 7.70s of the 7.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.01%)\n",
      "\t0.6946\t = Validation score   (accuracy)\n",
      "\t32.62s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 359.86s of the -28.85s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.7183\t = Validation score   (accuracy)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 389.16s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4213.4 rows/s (300 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2400 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/AutogluonModels/ag-20250522_155908\")\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250522_160537\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.0\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2\n",
      "CPU Count:          64\n",
      "Memory Avail:       374.19 GB / 503.54 GB (74.3%)\n",
      "Disk Space Avail:   33778.96 GB / 51214.59 GB (66.0%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ... Time limit = 360s\n",
      "AutoGluon will save models to \"/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/AutogluonModels/ag-20250522_160537\"\n",
      "Train Data Rows:    2400\n",
      "Train Data Columns: 17\n",
      "Label Column:       status\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = successful, class 0 = failed\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (successful) vs negative (failed) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    383168.44 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.49 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['name']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 30\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])        : 4 | ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "\t\t('int', [])          : 4 | ['blurb_length', 'name_length', 'start_month', 'end_month']\n",
      "\t\t('object', [])       : 8 | ['currency', 'main_category', 'sub_category', 'city', 'state', ...]\n",
      "\t\t('object', ['text']) : 1 | ['name']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  8 | ['currency', 'main_category', 'sub_category', 'city', 'state', ...]\n",
      "\t\t('float', [])                       :  4 | ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "\t\t('int', [])                         :  4 | ['blurb_length', 'name_length', 'start_month', 'end_month']\n",
      "\t\t('int', ['binned', 'text_special']) : 20 | ['name.char_count', 'name.word_count', 'name.capital_ratio', 'name.lower_ratio', 'name.digit_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 31 | ['__nlp__.album', '__nlp__.an', '__nlp__.and', '__nlp__.art', '__nlp__.book', ...]\n",
      "\t1.1s = Fit runtime\n",
      "\t17 features in original data used to generate 67 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.15s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 358.85s of the 358.83s of remaining time.\n",
      "\t0.5979\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 358.61s of the 358.58s of remaining time.\n",
      "\t0.5821\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 358.49s of the 358.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7767\t = Validation score   (accuracy)\n",
      "\t35.53s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 319.42s of the 319.39s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7825\t = Validation score   (accuracy)\n",
      "\t35.6s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 280.30s of the 280.28s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.68\t = Validation score   (accuracy)\n",
      "\t1.27s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 278.48s of the 278.46s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6875\t = Validation score   (accuracy)\n",
      "\t1.23s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 276.70s of the 276.67s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.12%)\n",
      "2025-05-22 18:07:05,787\tERROR serialization.py:462 -- Failed to unpickle serialized exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tSystem error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 931, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.RaySystemError: System error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 271.81s of the 271.79s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.67\t = Validation score   (accuracy)\n",
      "\t1.19s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 269.98s of the 269.96s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6733\t = Validation score   (accuracy)\n",
      "\t1.15s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 268.21s of the 268.19s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7546\t = Validation score   (accuracy)\n",
      "\t101.37s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 164.11s of the 164.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.01%)\n",
      "\t0.7875\t = Validation score   (accuracy)\n",
      "\t40.46s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 120.11s of the 120.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7312\t = Validation score   (accuracy)\n",
      "\t138.79s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 358.85s of the -22.45s of remaining time.\n",
      "\tEnsemble Weights: {'XGBoost_BAG_L1': 1.0}\n",
      "\t0.7875\t = Validation score   (accuracy)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 382.76s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2240.3 rows/s (300 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2400 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/AutogluonModels/ag-20250522_160537\")\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250522_161201\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.0\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2\n",
      "CPU Count:          64\n",
      "Memory Avail:       373.94 GB / 503.54 GB (74.3%)\n",
      "Disk Space Avail:   33778.70 GB / 51214.59 GB (66.0%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ... Time limit = 360s\n",
      "AutoGluon will save models to \"/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/AutogluonModels/ag-20250522_161201\"\n",
      "Train Data Rows:    2400\n",
      "Train Data Columns: 13\n",
      "Label Column:       status\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = successful, class 0 = failed\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (successful) vs negative (failed) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    382941.84 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.84 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 4 | ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "\t\t('int', [])    : 4 | ['blurb_length', 'name_length', 'start_month', 'end_month']\n",
      "\t\t('object', []) : 5 | ['currency', 'main_category', 'country', 'start_Q', 'end_Q']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 5 | ['currency', 'main_category', 'country', 'start_Q', 'end_Q']\n",
      "\t\t('float', [])    : 4 | ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "\t\t('int', [])      : 4 | ['blurb_length', 'name_length', 'start_month', 'end_month']\n",
      "\t0.1s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 359.86s of the 359.84s of remaining time.\n",
      "\t0.5979\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 359.76s of the 359.74s of remaining time.\n",
      "\t0.5821\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 359.68s of the 359.66s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7146\t = Validation score   (accuracy)\n",
      "\t33.94s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 322.24s of the 322.22s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7188\t = Validation score   (accuracy)\n",
      "\t32.68s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 286.10s of the 286.08s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.66\t = Validation score   (accuracy)\n",
      "\t1.18s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 284.38s of the 284.35s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6663\t = Validation score   (accuracy)\n",
      "\t1.14s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 282.72s of the 282.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.10%)\n",
      "2025-05-22 18:13:23,408\tERROR serialization.py:462 -- Failed to unpickle serialized exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tSystem error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 931, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.RaySystemError: System error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 277.95s of the 277.93s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6396\t = Validation score   (accuracy)\n",
      "\t1.12s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 276.15s of the 276.13s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6471\t = Validation score   (accuracy)\n",
      "\t1.1s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 274.40s of the 274.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.6837\t = Validation score   (accuracy)\n",
      "\t98.5s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 173.23s of the 173.21s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7067\t = Validation score   (accuracy)\n",
      "\t33.08s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 136.69s of the 136.67s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.6979\t = Validation score   (accuracy)\n",
      "\t151.88s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 359.87s of the -18.98s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.7188\t = Validation score   (accuracy)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 379.3s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4916.9 rows/s (300 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2400 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/AutogluonModels/ag-20250522_161201\")\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250522_161821\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.0\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2\n",
      "CPU Count:          64\n",
      "Memory Avail:       373.79 GB / 503.54 GB (74.2%)\n",
      "Disk Space Avail:   33778.20 GB / 51214.59 GB (66.0%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ... Time limit = 360s\n",
      "AutoGluon will save models to \"/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/AutogluonModels/ag-20250522_161821\"\n",
      "Train Data Rows:    2400\n",
      "Train Data Columns: 17\n",
      "Label Column:       status\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = successful, class 0 = failed\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (successful) vs negative (failed) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    382750.43 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.49 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['name']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 31\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])        : 4 | ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "\t\t('int', [])          : 4 | ['blurb_length', 'name_length', 'start_month', 'end_month']\n",
      "\t\t('object', [])       : 8 | ['currency', 'main_category', 'sub_category', 'city', 'state', ...]\n",
      "\t\t('object', ['text']) : 1 | ['name']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  8 | ['currency', 'main_category', 'sub_category', 'city', 'state', ...]\n",
      "\t\t('float', [])                       :  4 | ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "\t\t('int', [])                         :  4 | ['blurb_length', 'name_length', 'start_month', 'end_month']\n",
      "\t\t('int', ['binned', 'text_special']) : 20 | ['name.char_count', 'name.word_count', 'name.capital_ratio', 'name.lower_ratio', 'name.digit_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 32 | ['__nlp__.album', '__nlp__.an', '__nlp__.and', '__nlp__.art', '__nlp__.book', ...]\n",
      "\t1.3s = Fit runtime\n",
      "\t17 features in original data used to generate 68 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.37 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.36s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 358.64s of the 358.62s of remaining time.\n",
      "\t0.5925\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 358.51s of the 358.49s of remaining time.\n",
      "\t0.5896\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 358.40s of the 358.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7825\t = Validation score   (accuracy)\n",
      "\t37.08s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 317.92s of the 317.90s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7863\t = Validation score   (accuracy)\n",
      "\t37.01s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 277.35s of the 277.32s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6879\t = Validation score   (accuracy)\n",
      "\t1.26s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 275.53s of the 275.50s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6887\t = Validation score   (accuracy)\n",
      "\t1.21s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 273.76s of the 273.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.12%)\n",
      "2025-05-22 18:19:53,292\tERROR serialization.py:462 -- Failed to unpickle serialized exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tSystem error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 931, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.RaySystemError: System error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 267.84s of the 267.82s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6587\t = Validation score   (accuracy)\n",
      "\t1.15s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 265.99s of the 265.97s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6625\t = Validation score   (accuracy)\n",
      "\t1.14s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 264.20s of the 264.17s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7646\t = Validation score   (accuracy)\n",
      "\t104.89s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 156.69s of the 156.67s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.01%)\n",
      "\t0.7783\t = Validation score   (accuracy)\n",
      "\t39.91s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 113.21s of the 113.19s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7438\t = Validation score   (accuracy)\n",
      "\t138.66s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 358.64s of the -29.32s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.7863\t = Validation score   (accuracy)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 389.63s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3291.3 rows/s (300 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2400 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/AutogluonModels/ag-20250522_161821\")\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250522_162451\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.0\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2\n",
      "CPU Count:          64\n",
      "Memory Avail:       373.63 GB / 503.54 GB (74.2%)\n",
      "Disk Space Avail:   33777.94 GB / 51214.59 GB (66.0%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ... Time limit = 360s\n",
      "AutoGluon will save models to \"/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/AutogluonModels/ag-20250522_162451\"\n",
      "Train Data Rows:    2400\n",
      "Train Data Columns: 13\n",
      "Label Column:       status\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = successful, class 0 = failed\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (successful) vs negative (failed) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    382593.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.84 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 4 | ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "\t\t('int', [])    : 4 | ['blurb_length', 'name_length', 'start_month', 'end_month']\n",
      "\t\t('object', []) : 5 | ['currency', 'main_category', 'country', 'start_Q', 'end_Q']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 5 | ['currency', 'main_category', 'country', 'start_Q', 'end_Q']\n",
      "\t\t('float', [])    : 4 | ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "\t\t('int', [])      : 4 | ['blurb_length', 'name_length', 'start_month', 'end_month']\n",
      "\t0.1s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.15s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 359.85s of the 359.83s of remaining time.\n",
      "\t0.5925\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 359.76s of the 359.74s of remaining time.\n",
      "\t0.5896\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 359.68s of the 359.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7179\t = Validation score   (accuracy)\n",
      "\t33.47s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 322.71s of the 322.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7254\t = Validation score   (accuracy)\n",
      "\t34.2s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 284.84s of the 284.82s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6642\t = Validation score   (accuracy)\n",
      "\t1.19s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 283.11s of the 283.09s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6754\t = Validation score   (accuracy)\n",
      "\t1.16s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 281.44s of the 281.42s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.10%)\n",
      "2025-05-22 18:26:14,750\tERROR serialization.py:462 -- Failed to unpickle serialized exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tSystem error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 931, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.RaySystemError: System error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 276.59s of the 276.57s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6492\t = Validation score   (accuracy)\n",
      "\t1.13s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 274.79s of the 274.77s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6533\t = Validation score   (accuracy)\n",
      "\t1.1s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 272.96s of the 272.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.6987\t = Validation score   (accuracy)\n",
      "\t99.54s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 170.73s of the 170.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7225\t = Validation score   (accuracy)\n",
      "\t32.67s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 134.28s of the 134.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.6958\t = Validation score   (accuracy)\n",
      "\t150.56s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 359.85s of the -20.16s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.7254\t = Validation score   (accuracy)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 380.47s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 5459.7 rows/s (300 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2400 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/AutogluonModels/ag-20250522_162451\")\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250522_163112\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.0\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2\n",
      "CPU Count:          64\n",
      "Memory Avail:       373.18 GB / 503.54 GB (74.1%)\n",
      "Disk Space Avail:   33777.54 GB / 51214.59 GB (66.0%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ... Time limit = 360s\n",
      "AutoGluon will save models to \"/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/AutogluonModels/ag-20250522_163112\"\n",
      "Train Data Rows:    2400\n",
      "Train Data Columns: 17\n",
      "Label Column:       status\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = successful, class 0 = failed\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (successful) vs negative (failed) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    382116.65 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.49 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['name']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 30\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])        : 4 | ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "\t\t('int', [])          : 4 | ['blurb_length', 'name_length', 'start_month', 'end_month']\n",
      "\t\t('object', [])       : 8 | ['currency', 'main_category', 'sub_category', 'city', 'state', ...]\n",
      "\t\t('object', ['text']) : 1 | ['name']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  8 | ['currency', 'main_category', 'sub_category', 'city', 'state', ...]\n",
      "\t\t('float', [])                       :  4 | ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "\t\t('int', [])                         :  4 | ['blurb_length', 'name_length', 'start_month', 'end_month']\n",
      "\t\t('int', ['binned', 'text_special']) : 20 | ['name.char_count', 'name.word_count', 'name.capital_ratio', 'name.lower_ratio', 'name.digit_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 31 | ['__nlp__.album', '__nlp__.an', '__nlp__.and', '__nlp__.art', '__nlp__.book', ...]\n",
      "\t1.3s = Fit runtime\n",
      "\t17 features in original data used to generate 67 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.3s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 358.70s of the 358.68s of remaining time.\n",
      "\t0.585\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 358.58s of the 358.56s of remaining time.\n",
      "\t0.5779\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 358.47s of the 358.44s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7854\t = Validation score   (accuracy)\n",
      "\t36.78s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 318.22s of the 318.19s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7917\t = Validation score   (accuracy)\n",
      "\t37.91s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 276.83s of the 276.80s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.7004\t = Validation score   (accuracy)\n",
      "\t1.26s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 275.02s of the 274.99s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6946\t = Validation score   (accuracy)\n",
      "\t1.19s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 273.24s of the 273.22s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.12%)\n",
      "2025-05-22 18:32:44,147\tERROR serialization.py:462 -- Failed to unpickle serialized exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tSystem error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 931, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.RaySystemError: System error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 268.23s of the 268.20s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6787\t = Validation score   (accuracy)\n",
      "\t1.18s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 266.38s of the 266.35s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6779\t = Validation score   (accuracy)\n",
      "\t1.16s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 264.57s of the 264.55s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7646\t = Validation score   (accuracy)\n",
      "\t105.43s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 156.44s of the 156.42s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.01%)\n",
      "\t0.7908\t = Validation score   (accuracy)\n",
      "\t42.17s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 110.74s of the 110.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7308\t = Validation score   (accuracy)\n",
      "\t138.2s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 358.70s of the -31.43s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.7917\t = Validation score   (accuracy)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 391.74s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3739.6 rows/s (300 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2400 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/AutogluonModels/ag-20250522_163112\")\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250522_163744\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.0\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2\n",
      "CPU Count:          64\n",
      "Memory Avail:       373.26 GB / 503.54 GB (74.1%)\n",
      "Disk Space Avail:   33777.01 GB / 51214.59 GB (66.0%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ... Time limit = 360s\n",
      "AutoGluon will save models to \"/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/AutogluonModels/ag-20250522_163744\"\n",
      "Train Data Rows:    2400\n",
      "Train Data Columns: 13\n",
      "Label Column:       status\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = successful, class 0 = failed\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (successful) vs negative (failed) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    382200.32 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.84 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 4 | ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "\t\t('int', [])    : 4 | ['blurb_length', 'name_length', 'start_month', 'end_month']\n",
      "\t\t('object', []) : 5 | ['currency', 'main_category', 'country', 'start_Q', 'end_Q']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 5 | ['currency', 'main_category', 'country', 'start_Q', 'end_Q']\n",
      "\t\t('float', [])    : 4 | ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "\t\t('int', [])      : 4 | ['blurb_length', 'name_length', 'start_month', 'end_month']\n",
      "\t0.1s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 359.86s of the 359.84s of remaining time.\n",
      "\t0.585\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 359.76s of the 359.74s of remaining time.\n",
      "\t0.5779\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 359.68s of the 359.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7221\t = Validation score   (accuracy)\n",
      "\t33.09s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 323.03s of the 323.01s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.725\t = Validation score   (accuracy)\n",
      "\t33.29s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 286.19s of the 286.17s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6729\t = Validation score   (accuracy)\n",
      "\t1.14s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 284.50s of the 284.48s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6713\t = Validation score   (accuracy)\n",
      "\t1.16s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 282.81s of the 282.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.10%)\n",
      "2025-05-22 18:39:06,698\tERROR serialization.py:462 -- Failed to unpickle serialized exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tSystem error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 931, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.RaySystemError: System error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 278.07s of the 278.05s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6558\t = Validation score   (accuracy)\n",
      "\t1.14s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 276.26s of the 276.24s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6592\t = Validation score   (accuracy)\n",
      "\t1.1s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 274.51s of the 274.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7013\t = Validation score   (accuracy)\n",
      "\t99.45s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 172.43s of the 172.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7158\t = Validation score   (accuracy)\n",
      "\t33.6s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 135.29s of the 135.27s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7029\t = Validation score   (accuracy)\n",
      "\t152.7s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 359.86s of the -21.32s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.725\t = Validation score   (accuracy)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 381.63s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4473.6 rows/s (300 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2400 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/AutogluonModels/ag-20250522_163744\")\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250522_164406\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.0\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2\n",
      "CPU Count:          64\n",
      "Memory Avail:       373.49 GB / 503.54 GB (74.2%)\n",
      "Disk Space Avail:   33774.95 GB / 51214.59 GB (65.9%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ... Time limit = 360s\n",
      "AutoGluon will save models to \"/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/AutogluonModels/ag-20250522_164406\"\n",
      "Train Data Rows:    2400\n",
      "Train Data Columns: 17\n",
      "Label Column:       status\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = successful, class 0 = failed\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (successful) vs negative (failed) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    382447.34 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.49 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['name']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 30\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])        : 4 | ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "\t\t('int', [])          : 4 | ['blurb_length', 'name_length', 'start_month', 'end_month']\n",
      "\t\t('object', [])       : 8 | ['currency', 'main_category', 'sub_category', 'city', 'state', ...]\n",
      "\t\t('object', ['text']) : 1 | ['name']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  8 | ['currency', 'main_category', 'sub_category', 'city', 'state', ...]\n",
      "\t\t('float', [])                       :  4 | ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "\t\t('int', [])                         :  4 | ['blurb_length', 'name_length', 'start_month', 'end_month']\n",
      "\t\t('int', ['binned', 'text_special']) : 20 | ['name.char_count', 'name.word_count', 'name.capital_ratio', 'name.lower_ratio', 'name.digit_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 31 | ['__nlp__.album', '__nlp__.an', '__nlp__.and', '__nlp__.art', '__nlp__.book', ...]\n",
      "\t1.2s = Fit runtime\n",
      "\t17 features in original data used to generate 67 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.28s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 358.72s of the 358.70s of remaining time.\n",
      "\t0.59\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 358.54s of the 358.52s of remaining time.\n",
      "\t0.5896\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 358.43s of the 358.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7867\t = Validation score   (accuracy)\n",
      "\t36.68s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 318.19s of the 318.17s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7921\t = Validation score   (accuracy)\n",
      "\t36.1s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 278.53s of the 278.51s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6971\t = Validation score   (accuracy)\n",
      "\t1.25s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 276.71s of the 276.69s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.7046\t = Validation score   (accuracy)\n",
      "\t1.24s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 274.94s of the 274.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.12%)\n",
      "2025-05-22 18:45:36,913\tERROR serialization.py:462 -- Failed to unpickle serialized exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tSystem error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 931, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.RaySystemError: System error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 269.95s of the 269.93s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6713\t = Validation score   (accuracy)\n",
      "\t1.17s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 268.10s of the 268.08s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6792\t = Validation score   (accuracy)\n",
      "\t1.17s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 266.27s of the 266.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7679\t = Validation score   (accuracy)\n",
      "\t107.16s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 156.50s of the 156.48s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.01%)\n",
      "\t0.7783\t = Validation score   (accuracy)\n",
      "\t41.09s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 111.83s of the 111.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7292\t = Validation score   (accuracy)\n",
      "\t139.01s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 358.72s of the -30.94s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.7921\t = Validation score   (accuracy)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 391.24s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4171.6 rows/s (300 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2400 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/AutogluonModels/ag-20250522_164406\")\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250522_165038\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.0\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2\n",
      "CPU Count:          64\n",
      "Memory Avail:       373.70 GB / 503.54 GB (74.2%)\n",
      "Disk Space Avail:   33778.37 GB / 51214.59 GB (66.0%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ... Time limit = 360s\n",
      "AutoGluon will save models to \"/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/AutogluonModels/ag-20250522_165038\"\n",
      "Train Data Rows:    2400\n",
      "Train Data Columns: 13\n",
      "Label Column:       status\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = successful, class 0 = failed\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (successful) vs negative (failed) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    382643.80 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.84 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 4 | ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "\t\t('int', [])    : 4 | ['blurb_length', 'name_length', 'start_month', 'end_month']\n",
      "\t\t('object', []) : 5 | ['currency', 'main_category', 'country', 'start_Q', 'end_Q']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 5 | ['currency', 'main_category', 'country', 'start_Q', 'end_Q']\n",
      "\t\t('float', [])    : 4 | ['launched_at', 'deadline', 'duration', 'goal_usd']\n",
      "\t\t('int', [])      : 4 | ['blurb_length', 'name_length', 'start_month', 'end_month']\n",
      "\t0.1s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.16 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 359.86s of the 359.84s of remaining time.\n",
      "\t0.59\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 359.78s of the 359.76s of remaining time.\n",
      "\t0.5896\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 359.70s of the 359.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7275\t = Validation score   (accuracy)\n",
      "\t35.13s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 321.21s of the 321.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7242\t = Validation score   (accuracy)\n",
      "\t33.25s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 284.45s of the 284.43s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6779\t = Validation score   (accuracy)\n",
      "\t1.17s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 282.75s of the 282.73s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6646\t = Validation score   (accuracy)\n",
      "\t1.16s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 281.01s of the 280.98s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.10%)\n",
      "2025-05-22 18:52:02,532\tERROR serialization.py:462 -- Failed to unpickle serialized exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tSystem error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 931, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.RaySystemError: System error: Failed to unpickle serialized exception\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "    return pickle.loads(ray_exception.serialized_exception)\n",
      "ModuleNotFoundError: No module named '_catboost'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "    return RayError.from_bytes(obj)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "    return RayError.from_ray_exception(ray_exception)\n",
      "  File \"/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Failed to unpickle serialized exception\n",
      "\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 276.14s of the 276.12s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6583\t = Validation score   (accuracy)\n",
      "\t1.14s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 274.36s of the 274.34s of remaining time.\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "\t0.6658\t = Validation score   (accuracy)\n",
      "\t1.1s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 272.62s of the 272.60s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.6971\t = Validation score   (accuracy)\n",
      "\t100.99s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 169.05s of the 169.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.7121\t = Validation score   (accuracy)\n",
      "\t33.83s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 131.67s of the 131.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=8, gpus=1, memory=0.00%)\n",
      "\t0.6975\t = Validation score   (accuracy)\n",
      "\t142.4s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 359.86s of the -14.58s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.7275\t = Validation score   (accuracy)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 374.88s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 5045.9 rows/s (300 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2400 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/guptaa/anshul/FreeText_TaBench/datasets_notebooks/classification/AutogluonModels/ag-20250522_165038\")\n",
      "/work/dlclarge2/guptaa-dataset/anshul_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from baseline_eval import evaluate_baseline, plot_model_performance_summary\n",
    "results = evaluate_baseline(\n",
    "    df=loaded_df,\n",
    "    model='AGTabular',\n",
    "    df_name=dataset_config['dataset_name'],\n",
    "    label_col= dataset_config['target'],\n",
    "    task_type=dataset_config['task'],\n",
    "    textual_cols=textual_cols,\n",
    "    k_folds=5,\n",
    "    seed=0,\n",
    "    max_samples=3000,\n",
    "    output_path=f\"../../baseline_results/{dataset_config['task']}/{dataset_config['dataset_name']}.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aff4fd2d-6bc0-41e4-aa0e-a6fa7b3febee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving plot to ../../baseline_results/plots/clf/score\n",
      "Saving plot to ../../baseline_results/plots/clf/loss\n",
      "Saving plot to ../../baseline_results/plots/clf/roc_auc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score':                                     mean       std\n",
       " model                                             \n",
       " AutoGluon_Tabular_without_text  0.699333  0.019882\n",
       " AutoGluon_Tabular_with_text     0.776000  0.019741,\n",
       " 'loss':                                     mean       std\n",
       " model                                             \n",
       " AutoGluon_Tabular_with_text     0.461863  0.020929\n",
       " AutoGluon_Tabular_without_text  0.568149  0.017239,\n",
       " 'roc_auc':                                     mean       std\n",
       " model                                             \n",
       " AutoGluon_Tabular_without_text  0.776809  0.020199\n",
       " AutoGluon_Tabular_with_text     0.862541  0.013207}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = pd.read_csv(f\"../../baseline_results/{dataset_config['task']}/{dataset_config['dataset_name']}.csv\")\n",
    "plot_model_performance_summary(name=dataset_config['dataset_name'],task=dataset_config['task'], df=rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6c51ad-71b5-4dde-a3c9-b2c03c97af0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
